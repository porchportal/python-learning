{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/porchportal/python-learning/blob/main/WebChat_OpenThaiGPT_1_0_0_beta_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "hxLVa0cbqEwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-tJ5jaTbe21x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw-qqzCDcFPW"
      },
      "source": [
        "## Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml0R3IoDAjs5"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTurBr1GMUGR"
      },
      "source": [
        "# Dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRUBK5K2EuCk"
      },
      "outputs": [],
      "source": [
        "!pip install gradio transformers[sentencepiece] git+https://github.com/huggingface/peft.git accelerate bitsandbytes loralib fire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCBRyhlfYNuc"
      },
      "source": [
        "# Inference Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Helpers to support streaming generate output.\n",
        "Borrowed from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/callbacks.py\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n"
      ],
      "metadata": {
        "id": "8zypTlpja-z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKNgVzv4NLi3"
      },
      "source": [
        "# Class Stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90Bwz4-2Ky-p"
      },
      "outputs": [],
      "source": [
        "class Stream(transformers.StoppingCriteria):\n",
        "    def __init__(self, callback_func=None):\n",
        "        self.callback_func = callback_func\n",
        "\n",
        "    def __call__(self, input_ids, scores) -> bool:\n",
        "        if self.callback_func is not None:\n",
        "            self.callback_func(input_ids[0])\n",
        "        return False\n",
        "\n",
        "class Iteratorize:\n",
        "    \"\"\"\n",
        "    Transforms a function that takes a callback\n",
        "    into a lazy iterator (generator).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, func, kwargs={}, callback=None):\n",
        "        self.mfunc = func\n",
        "        self.c_callback = callback\n",
        "        self.q = Queue()\n",
        "        self.sentinel = object()\n",
        "        self.kwargs = kwargs\n",
        "        self.stop_now = False\n",
        "\n",
        "        def _callback(val):\n",
        "            if self.stop_now:\n",
        "                raise ValueError\n",
        "            self.q.put(val)\n",
        "\n",
        "        def gentask():\n",
        "            try:\n",
        "                ret = self.mfunc(callback=_callback, **self.kwargs)\n",
        "            except ValueError:\n",
        "                pass\n",
        "            except:\n",
        "                traceback.print_exc()\n",
        "                pass\n",
        "\n",
        "            self.q.put(self.sentinel)\n",
        "            if self.c_callback:\n",
        "                self.c_callback(ret)\n",
        "\n",
        "        self.thread = Thread(target=gentask)\n",
        "        self.thread.start()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        obj = self.q.get(True, None)\n",
        "        if obj is self.sentinel:\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.stop_now = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcwwkNbANQN7"
      },
      "source": [
        "# Class Prompter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0vgyf5MLIV1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A dedicated helper to manage templates and prompt building.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os.path as osp\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "class Prompter(object):\n",
        "    __slots__ = (\"template\", \"_verbose\")\n",
        "\n",
        "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
        "        self._verbose = verbose\n",
        "        template_name = \"alpaca\"\n",
        "        self.template = {\n",
        "            \"description\": \"Template used by Alpaca-LoRA.\",\n",
        "            \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
        "            \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
        "            \"response_split\": \"### Response:\"\n",
        "        }\n",
        "        if self._verbose:\n",
        "            print(\n",
        "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
        "            )\n",
        "\n",
        "    def generate_prompt(\n",
        "        self,\n",
        "        instruction: str,\n",
        "        input: Union[None, str] = None,\n",
        "        label: Union[None, str] = None,\n",
        "    ) -> str:\n",
        "        # returns the full prompt from instruction and optional input\n",
        "        # if a label (=response, =output) is provided, it's also appended.\n",
        "        if input:\n",
        "            res = self.template[\"prompt_input\"].format(\n",
        "                instruction=instruction, input=input\n",
        "            )\n",
        "        else:\n",
        "            res = self.template[\"prompt_no_input\"].format(\n",
        "                instruction=instruction\n",
        "            )\n",
        "        if label:\n",
        "            res = f\"{res}{label}\"\n",
        "        if self._verbose:\n",
        "            print(res)\n",
        "        return res\n",
        "\n",
        "    def get_response(self, output: str) -> str:\n",
        "        return output.split(self.template[\"response_split\"])[1].strip()\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:  # noqa: E722\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Base Model and Lora"
      ],
      "metadata": {
        "id": "t9Gsf1Req18C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsOjziA3Dppt"
      },
      "outputs": [],
      "source": [
        "base_model      = '/content/drive/MyDrive/ExportModel/pantip2_hf_ckpt'\n",
        "lora_weights    = ''\n",
        "load_8bit       = True\n",
        "\n",
        "prompt_template = \"\"\n",
        "server_name     = \"0.0.0.0\"\n",
        "share_gradio    = True\n",
        "\n",
        "prompter  = Prompter(prompt_template)\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "\n",
        "if device == \"cuda\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_8bit=load_8bit,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    if lora_weights:\n",
        "      model = PeftModel.from_pretrained(\n",
        "          model,\n",
        "          lora_weights,\n",
        "          torch_dtype=torch.float16,\n",
        "      )\n",
        "elif device == \"mps\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    if lora_weights:\n",
        "      model = PeftModel.from_pretrained(\n",
        "          model,\n",
        "          lora_weights,\n",
        "          device_map={\"\": device},\n",
        "          torch_dtype=torch.float16,\n",
        "      )\n",
        "else:\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
        "    )\n",
        "    if lora_weights:\n",
        "      model = PeftModel.from_pretrained(\n",
        "          model,\n",
        "          lora_weights,\n",
        "          device_map={\"\": device},\n",
        "      )\n",
        "\n",
        "# unwind broken decapoda-research config\n",
        "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
        "model.config.bos_token_id = 1\n",
        "model.config.eos_token_id = 2\n",
        "\n",
        "if not load_8bit:\n",
        "    model.half()  # seems to fix bugs for some users.\n",
        "\n",
        "model.eval()\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j4SLYUSMYya"
      },
      "source": [
        "## Evaluate by calling a function\n",
        "\n",
        "https://huggingface.co/docs/transformers/main_classes/text_generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Uz4O47UMJLW"
      },
      "outputs": [],
      "source": [
        "def text_evaluate(\n",
        "      instruction,\n",
        "      input=None,\n",
        "      temperature=0.1,\n",
        "      top_p=0.75,\n",
        "      top_k=40,\n",
        "      num_beams=1,\n",
        "      repetition_penalty=2,\n",
        "      no_repeat_ngram=5,\n",
        "      max_new_tokens=128,\n",
        "      stream_output=False,\n",
        "      **kwargs,\n",
        "  ):\n",
        "      prompt    = prompter.generate_prompt(instruction, input)\n",
        "      inputs    = tokenizer(prompt, return_tensors=\"pt\")\n",
        "      input_ids = inputs[\"input_ids\"].to(device)\n",
        "\n",
        "      generation_config = GenerationConfig(\n",
        "          temperature=temperature,\n",
        "          top_p=top_p,\n",
        "          top_k=top_k,\n",
        "          num_beams=num_beams,\n",
        "          **kwargs,\n",
        "      )\n",
        "\n",
        "      generate_params = {\n",
        "          \"input_ids\": input_ids,\n",
        "          \"generation_config\": generation_config,\n",
        "          \"return_dict_in_generate\": True,\n",
        "          \"output_scores\": True,\n",
        "          \"max_new_tokens\": max_new_tokens,\n",
        "      }\n",
        "\n",
        "      # Without streaming\n",
        "      with torch.no_grad():\n",
        "          generation_output = model.generate(\n",
        "              input_ids=input_ids,\n",
        "              generation_config=generation_config,\n",
        "              return_dict_in_generate=True,\n",
        "              output_scores=True,\n",
        "              max_new_tokens=max_new_tokens,\n",
        "          )\n",
        "      s = generation_output.sequences[0]\n",
        "      return tokenizer.decode(s).split(\"### Response:\")[1].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Input Test"
      ],
      "metadata": {
        "id": "jT8AZAou9WHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = '‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ'\n",
        "\n",
        "print(\"Question :\", question)\n",
        "print(\"Answer   :\", text_evaluate(question))"
      ],
      "metadata": {
        "id": "N3u7CVPU9UtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FPLU-x8NgKS"
      },
      "source": [
        "# Gradio Web Chat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()                # Clear GPU Ram"
      ],
      "metadata": {
        "id": "j_f-pTlS4cWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55bae99d-7a63-4a40-bab7-de7d10b8ab1b"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def format_chat_prompt(message, chat_history):\n",
        "    prompt = \"\"\n",
        "    for turn in chat_history:\n",
        "        user_message, bot_message = turn\n",
        "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
        "\n",
        "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
        "    return prompt\n",
        "\n",
        "def respond(message, chat_history):\n",
        "        formatted_prompt  = format_chat_prompt(message, chat_history)\n",
        "\n",
        "        bot_message = text_evaluate(message)\n",
        "        chat_history.append((message, bot_message))\n",
        "\n",
        "        return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # üáπüá≠ chatBot by OpenThaiGPT 1.0.0-beta\n",
        "\n",
        "        \"\"\"\n",
        "    )\n",
        "    #chatbot = gr.Chatbot(height=240)        # just to fit the notebook\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg     = gr.Textbox(label=\"Prompt\")\n",
        "    btn     = gr.Button(\"Submit\")\n",
        "    clear   = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
        "\n",
        "    btn.click( respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) # Press enter to submit\n",
        "gr.close_all()\n",
        "\n",
        "demo.launch(server_name=\"0.0.0.0\", share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "toc_visible": true,
      "collapsed_sections": [
        "yKNgVzv4NLi3",
        "tcwwkNbANQN7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}